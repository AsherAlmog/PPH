{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTdcb-IxCMwd"
      },
      "source": [
        "# PPH — TabM Training (Colab)\n",
        "\n",
        "This notebook trains the **TabM** model on engineered-window features for PPH prediction.\n",
        "\n",
        "**Before you run:**\n",
        "1. Upload your files to **Google Drive → MyDrive → PPH**:\n",
        "   - `features_all.parquet`\n",
        "   - `feature_columns.json`\n",
        "   - Labels in either `PPH/out_labels/labels_vector.parquet` **or** directly as `PPH/labels_vector.parquet`\n",
        "2. In Colab, go to **Runtime → Change runtime type → GPU** (optional but recommended).\n",
        "\n",
        "Then run the notebook top-to-bottom.\n"
      ],
      "id": "mTdcb-IxCMwd"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_OIvkYlCMwi",
        "outputId": "49adb384-876d-4080-d60a-485f1513ea29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies (TabM + numeric embeddings). Torch is preinstalled on Colab.\n",
        "!pip -q install tabm rtdl_num_embeddings pyarrow fastparquet\n",
        "import torch\n",
        "print('Torch version:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())\n"
      ],
      "id": "r_OIvkYlCMwi"
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive and set BASE_DIR\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os, json\n",
        "BASE_DIR = '/content/drive/MyDrive/PPH/try2'\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "print('Using BASE_DIR:', BASE_DIR)\n",
        "print('Contents of BASE_DIR:', os.listdir(BASE_DIR))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTaJT1aRCnkF",
        "outputId": "0881bd1b-785c-4681-d833-280d504b75e7"
      },
      "id": "TTaJT1aRCnkF",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Using BASE_DIR: /content/drive/MyDrive/PPH/try2\n",
            "Contents of BASE_DIR: ['features_rt_only.parquet', 'features_all.parquet', 'feature_columns_all.json', 'labels_all.parquet', 'feature_columns_rt_only.json', 'top_feature_hist']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgYhCKKgCMwk",
        "outputId": "980a23a5-23d3-4eae-b32b-54d16160a91c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features path: /content/drive/MyDrive/PPH/try2/features_all.parquet exists: True\n",
            "Labels path  : /content/drive/MyDrive/PPH/try2/labels_all.parquet exists: True\n",
            "Feature cols : /content/drive/MyDrive/PPH/try2/feature_columns_all.json exists: True\n"
          ]
        }
      ],
      "source": [
        "# Resolve paths\n",
        "FEATURES = os.path.join(BASE_DIR, 'features_all.parquet')\n",
        "FEATURE_COLS_JSON = os.path.join(BASE_DIR, 'feature_columns_all.json')\n",
        "LABELS = os.path.join(BASE_DIR, 'labels_all.parquet')\n",
        "\n",
        "print('Features path:', FEATURES, 'exists:', os.path.exists(FEATURES))\n",
        "print('Labels path  :', LABELS, 'exists:', os.path.exists(LABELS))\n",
        "print('Feature cols :', FEATURE_COLS_JSON, 'exists:', os.path.exists(FEATURE_COLS_JSON))\n",
        "assert os.path.exists(FEATURES), 'Missing features_all.parquet in PPH folder.'\n",
        "assert os.path.exists(LABELS), 'Missing labels_vector.parquet (either PPH/out_labels/... or PPH root).'\n",
        "assert os.path.exists(FEATURE_COLS_JSON), 'Missing feature_columns.json in PPH folder.'\n"
      ],
      "id": "KgYhCKKgCMwk"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "imports_and_cfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8c06179-2f29-4d4a-84f1-700039ce102d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Imports + Config\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, precision_recall_curve,\n",
        "    roc_curve, precision_recall_fscore_support, confusion_matrix,\n",
        ")\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tabm import TabM\n",
        "try:\n",
        "    from rtdl_num_embeddings import LinearReLUEmbeddings\n",
        "    _HAS_NUM_EMB = True\n",
        "except Exception:\n",
        "    _HAS_NUM_EMB = False\n",
        "\n",
        "pd.options.mode.copy_on_write = True\n",
        "\n",
        "CFG = {\n",
        "    'base_dir': BASE_DIR,\n",
        "    'features_all_path': FEATURES,\n",
        "    'labels_all_path': LABELS,\n",
        "    'feature_cols_json': FEATURE_COLS_JSON,\n",
        "    'keep_mothers_csv': None,\n",
        "    'keep_mothers_col': 'hashed_mother_id',\n",
        "    'target_col': 'label',\n",
        "    'labels_event_time_col': None,\n",
        "    'target_binarize': True,\n",
        "    'positive_labels': [1, 2],\n",
        "    'drop_na_target': True,\n",
        "    'max_minutes_postpartum': None,\n",
        "    'keep_only_name_contains': [\n",
        "        'sistol', 'diastol', 'bp - mean', 'map', 'pulse', 'saturation',\n",
        "        'shock_index', 'si_', 'shockindex', 'HGB', 'HCT', 'PP'\n",
        "    ],\n",
        "    'drop_realtime_measurements': False,\n",
        "    'realtime_measurement_patterns': [\n",
        "        'sistol', 'diastol', 'bp - mean', 'map', 'pulse', 'saturation', 'heat',\n",
        "        'hgb', 'hct', 'plt', 'fibrinogen', 'wbc',\n",
        "        'sodium_blood', 'creatinine_blood', 'uric_acid_blood',\n",
        "        '_fine_', '_coarse_',\n",
        "    ],\n",
        "    'drop_if_name_contains': ['delta_t'],\n",
        "    'duplicate_id_candidates': [( 'hashed_mother_id', ['mother_id'] )],\n",
        "    'use_feature_blocklist': True,\n",
        "    'hard_time_blocklist_patterns': (\n",
        "        't_from_birth_sec', 'mins_postpartum', 'episode_start',\n",
        "    ),\n",
        "    'leakage_blocklist_patterns': (\n",
        "        '_recency_s', '_measured', '_given', 'time_since_',\n",
        "    ),\n",
        "    'leakage_blocklist_columns': (\n",
        "        'anesthesia_local','anesthesia_epidural','anesthesia_general',\n",
        "        'anesthesia_spinal','no_anesthesia','amniofusion','oxytocin_administrations',\n",
        "        'membranes_rupture_type','amniotic_fluid',\n",
        "    ),\n",
        "    'drop_constant_features': True,\n",
        "    'min_feature_variance': 1e-12,\n",
        "    'validation_strategy': 'holdout',  # 'holdout' or 'group_kfold'\n",
        "    'holdout_group_frac': 0.2,\n",
        "    'group_kfold_splits': 5,\n",
        "    'random_state': 42,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'num_workers': 0,\n",
        "    'torch_num_threads': None,\n",
        "    'epochs': 5,\n",
        "    'batch_size': 1024,\n",
        "    'val_batch_size': 2048,\n",
        "    'lr': 2e-3,\n",
        "    'weight_decay': 3e-4,\n",
        "    'early_stop_patience': 5,\n",
        "    'tabm_k': 24,\n",
        "    'use_num_embeddings': True,\n",
        "    'out_model_pt':          os.path.join(BASE_DIR, 'tabm_model.pt'),\n",
        "    'out_used_features_json': os.path.join(BASE_DIR, 'used_feature_columns.json'),\n",
        "    'out_metrics_txt':        os.path.join(BASE_DIR, 'metrics.txt'),\n",
        "    'out_feat_csv':           os.path.join(BASE_DIR, 'feature_importances.csv'),\n",
        "    'out_plot_roc':           os.path.join(BASE_DIR, 'roc.png'),\n",
        "    'out_plot_pr':            os.path.join(BASE_DIR, 'pr.png'),\n",
        "    'out_plot_cm':            os.path.join(BASE_DIR, 'cm.png'),\n",
        "    'out_plot_feat':          os.path.join(BASE_DIR, 'feat_importance.png'),\n",
        "    'feat_top_n': 30,\n",
        "    'matrices_cache_enabled': False,\n",
        "    'matrices_force_recompute': False,\n",
        "    'matrices_cache_dir':   os.path.join(BASE_DIR, '.cache_tabm'),\n",
        "    'matrices_cache_X':     os.path.join(BASE_DIR, '.cache_tabm', 'Xdf.parquet'),\n",
        "    'matrices_cache_y':     os.path.join(BASE_DIR, '.cache_tabm', 'y.parquet'),\n",
        "    'matrices_cache_meta':  os.path.join(BASE_DIR, '.cache_tabm', 'meta.parquet'),\n",
        "    'matrices_cache_cols':  os.path.join(BASE_DIR, '.cache_tabm', 'used_cols.json'),\n",
        "    'downsample_training_to_even': False,\n",
        "    'downsample_random_state': 123,\n",
        "    'hist_top_n': 50,\n",
        "    'hist_bins': 40,\n",
        "    'out_hist_dir': os.path.join(BASE_DIR, 'top_feature_hist'),\n",
        "    'hist_plot_all_used': False,\n",
        "    'perm_importance_enabled': False,\n",
        "    'perm_importance_n_repeats': 5,\n",
        "    'perm_importance_max_features': 200,\n",
        "    # --- LR scheduler ---\n",
        "    'lr_scheduler': 'cosine',        # 'none' | 'cosine' | 'onecycle' | 'plateau'\n",
        "    'min_lr': 1e-5,                # floor LR for cosine / plateau\n",
        "    # OneCycle params\n",
        "    'onecycle_pct_start': 0.3,\n",
        "    'onecycle_div_factor': 25.0,\n",
        "    'onecycle_final_div_factor': 1e4,\n",
        "    # ReduceLROnPlateau params\n",
        "    'plateau_factor': 0.5,\n",
        "    'plateau_patience': 2,\n",
        "\n",
        "}\n",
        "print('Device:', CFG['device'])\n"
      ],
      "id": "imports_and_cfg"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CFG patch (paste right after your CFG definition) ---\n",
        "CFG.update({\n",
        "    # train longer and let early stopping decide\n",
        "    'epochs': 10,\n",
        "    'early_stop_patience': 4,\n",
        "\n",
        "    # LR scheduler: react to validation AP\n",
        "    'lr_scheduler': 'plateau',      # 'none' | 'cosine' | 'onecycle' | 'plateau'\n",
        "    'min_lr': 1e-5,\n",
        "    'plateau_factor': 0.5,\n",
        "    'plateau_patience': 2,\n",
        "\n",
        "    # a bit more capacity + regularization\n",
        "    'tabm_k': 24,                   # was 24\n",
        "    'weight_decay': 1e-3,           # was 3e-4\n",
        "\n",
        "    # imbalance + stability knobs\n",
        "    'max_pos_weight': 2000,         # cap extreme class weights\n",
        "    'use_weighted_sampler': False,   # keep all data via class-balanced sampling\n",
        "\n",
        "    # training stability\n",
        "    'grad_clip_norm': 1.0,          # set None to disable\n",
        "})\n"
      ],
      "metadata": {
        "id": "R1OfGX_oEu1w"
      },
      "id": "R1OfGX_oEu1w",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "helpers_utils"
      },
      "outputs": [],
      "source": [
        "# ==== Utilities, data I/O, feature planning ====\n",
        "import gc, math, random\n",
        "from typing import Optional, List, Tuple, Dict, Iterable\n",
        "\n",
        "TR_KEYS = ['hashed_mother_id', 'episode_idx', 'snapshot_time']\n",
        "PAIR_KEYS = ['hashed_mother_id', 'episode_idx']\n",
        "META_KEYS = ['hashed_mother_id', 'episode_idx', 'snapshot_time', 'pregnancy_index']\n",
        "LABEL_KEYSET = ['hashed_mother_id', 'episode_idx']\n",
        "\n",
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def set_torch_threads(n: Optional[int]):\n",
        "    if n is not None and isinstance(n, int) and n > 0:\n",
        "        torch.set_num_threads(n)\n",
        "\n",
        "def _load_keep_mothers_set(cfg: Dict) -> Optional[set]:\n",
        "    path = cfg.get('keep_mothers_csv')\n",
        "    if not path or not os.path.exists(path):\n",
        "        return None\n",
        "    df = pd.read_csv(path)\n",
        "    col = cfg.get('keep_mothers_col') or 'hashed_mother_id'\n",
        "    if col not in df.columns:\n",
        "        col = df.columns[0]\n",
        "    s = df[col].astype(str).str.strip()\n",
        "    s = s[s != '']\n",
        "    kept = set(s.unique().tolist())\n",
        "    print(f\"[INFO] keep_mothers_csv: keeping {len(kept):,} mothers.\")\n",
        "    return kept if kept else None\n",
        "\n",
        "def _drop_duplicate_id_columns(df: pd.DataFrame, cfg: Dict) -> pd.DataFrame:\n",
        "    for canon, dupes in cfg.get('duplicate_id_candidates', []):\n",
        "        if canon in df.columns:\n",
        "            to_drop = [d for d in dupes if d in df.columns]\n",
        "            if to_drop:\n",
        "                df = df.drop(columns=to_drop)\n",
        "    return df\n",
        "\n",
        "def _coerce_core_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if 'hashed_mother_id' in df:\n",
        "        if not isinstance(df['hashed_mother_id'].dtype, CategoricalDtype):\n",
        "            df['hashed_mother_id'] = df['hashed_mother_id'].astype('category')\n",
        "        else:\n",
        "            df['hashed_mother_id'] = df['hashed_mother_id'].cat.remove_unused_categories()\n",
        "    if 'episode_idx' in df:\n",
        "        df['episode_idx'] = pd.to_numeric(df['episode_idx'], errors='coerce').astype('Int32')\n",
        "    if 'pregnancy_index' in df:\n",
        "        df['pregnancy_index'] = pd.to_numeric(df['pregnancy_index'], errors='coerce').astype('Int64')\n",
        "    if 'snapshot_time' in df and not np.issubdtype(df['snapshot_time'].dtype, np.datetime64):\n",
        "        df['snapshot_time'] = pd.to_datetime(df['snapshot_time'], errors='coerce')\n",
        "    return df\n",
        "\n",
        "def _downcast_numeric_inplace(df: pd.DataFrame) -> None:\n",
        "    for c in df.columns:\n",
        "        s = df[c]\n",
        "        if pd.api.types.is_float_dtype(s):\n",
        "            df[c] = s.astype(np.float32)\n",
        "        elif pd.api.types.is_integer_dtype(s) and not pd.api.types.is_unsigned_integer_dtype(s):\n",
        "            if pd.isna(s).any():\n",
        "                try:\n",
        "                    df[c] = pd.to_numeric(s, errors='coerce').astype('Int32')\n",
        "                except Exception:\n",
        "                    pass\n",
        "            else:\n",
        "                df[c] = pd.to_numeric(s, downcast='integer')\n",
        "\n",
        "def _is_realtime_feature(name: str, cfg: Dict) -> bool:\n",
        "    low = name.lower()\n",
        "    pats = [p.lower() for p in (cfg.get('realtime_measurement_patterns') or [])]\n",
        "    return any(p in low for p in pats)\n",
        "\n",
        "def _name_is_dropped_by_metric(name: str, cfg: Dict) -> bool:\n",
        "    drops = tuple(cfg.get('drop_if_name_contains', []) or [])\n",
        "    if not drops:\n",
        "        return False\n",
        "    low = name.lower()\n",
        "    return any(s.lower() in low for s in drops)\n",
        "\n",
        "def _apply_keep_only_filter(cols: List[str], cfg: Dict) -> List[str]:\n",
        "    keeps = cfg.get('keep_only_name_contains', []) or []\n",
        "    if not keeps:\n",
        "        return cols\n",
        "    ks = [k.lower() for k in keeps]\n",
        "    kept = []\n",
        "    for c in cols:\n",
        "        if not _is_realtime_feature(c, cfg):\n",
        "            kept.append(c)\n",
        "        else:\n",
        "            if any(k in c.lower() for k in ks):\n",
        "                kept.append(c)\n",
        "    if not kept:\n",
        "        print('[WARN] keep_only_name_contains removed everything; falling back to original set.')\n",
        "        return cols\n",
        "    print(f\"[INFO] keep_only_name_contains kept {len(kept)}/{len(cols)} columns (static always kept).\")\n",
        "    return kept\n",
        "\n",
        "def _drop_realtime_from_list(cols: Iterable[str], cfg: Dict) -> List[str]:\n",
        "    cols = list(cols)\n",
        "    out = [c for c in cols if not _is_realtime_feature(c, cfg)]\n",
        "    print(f\"[INFO] drop_realtime_measurements removed {len(cols) - len(out)} real-time columns; kept {len(out)} static columns.\")\n",
        "    return out\n",
        "\n",
        "def _blocklist_filter(cols: List[str], cfg: Dict) -> List[str]:\n",
        "    if not cfg.get('use_feature_blocklist', True):\n",
        "        return [c for c in cols if not _name_is_dropped_by_metric(c, cfg)]\n",
        "    pats = tuple(cfg.get('leakage_blocklist_patterns', ())) + tuple(cfg.get('hard_time_blocklist_patterns', ()))\n",
        "    bad_cols = set(cfg.get('leakage_blocklist_columns', ()))\n",
        "    kept = []\n",
        "    for c in cols:\n",
        "        if c in bad_cols:\n",
        "            continue\n",
        "        if any(p in c for p in pats):\n",
        "            continue\n",
        "        if _name_is_dropped_by_metric(c, cfg):\n",
        "            continue\n",
        "        kept.append(c)\n",
        "    return kept\n",
        "\n",
        "def _plan_feature_columns_to_read(cfg: Dict) -> List[str]:\n",
        "    with open(cfg['feature_cols_json'], 'r') as f:\n",
        "        engineered_cols: List[str] = json.load(f)\n",
        "    forbid = {'hashed_mother_id','episode_idx','snapshot_time','episode_start','mins_postpartum','pregnancy_index'}\n",
        "    candidates = [c for c in engineered_cols if c not in forbid]\n",
        "    if cfg.get('drop_realtime_measurements', False):\n",
        "        candidates = _drop_realtime_from_list(candidates, cfg)\n",
        "    else:\n",
        "        candidates = _apply_keep_only_filter(candidates, cfg)\n",
        "    used = _blocklist_filter(candidates, cfg)\n",
        "    return used\n",
        "\n",
        "def load_feature_label_tables(cfg: Dict) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    keep_ids = _load_keep_mothers_set(cfg)\n",
        "    feat_cols = _plan_feature_columns_to_read(cfg)\n",
        "    feat_keep = list(dict.fromkeys(feat_cols + META_KEYS))\n",
        "    try:\n",
        "        X = pd.read_parquet(cfg['features_all_path'], columns=feat_keep)\n",
        "    except Exception:\n",
        "        X = pd.read_parquet(cfg['features_all_path'])\n",
        "    if keep_ids:\n",
        "        if 'hashed_mother_id' not in X.columns:\n",
        "            raise ValueError(\"features missing 'hashed_mother_id' but keep_mothers_csv is set.\")\n",
        "        before = len(X)\n",
        "        X = X[X['hashed_mother_id'].astype(str).isin(keep_ids)]\n",
        "        print(f\"[INFO] features: kept {len(X):,}/{before:,} rows after mother filter.\")\n",
        "    keepX = [c for c in feat_keep if c in X.columns]\n",
        "    if len(keepX) != len(X.columns):\n",
        "        X = X.loc[:, keepX]\n",
        "    X = _drop_duplicate_id_columns(X, CFG)\n",
        "    X = _coerce_core_dtypes(X)\n",
        "    _downcast_numeric_inplace(X)\n",
        "\n",
        "    y_cols = list(dict.fromkeys(LABEL_KEYSET + [cfg['target_col']]))\n",
        "    tcol = cfg.get('labels_event_time_col') or 'snapshot_time'\n",
        "    y_cols.append(tcol)\n",
        "    try:\n",
        "        Y = pd.read_parquet(cfg['labels_all_path'], columns=[c for c in y_cols if c is not None])\n",
        "    except Exception:\n",
        "        Y = pd.read_parquet(cfg['labels_all_path'])\n",
        "    if keep_ids:\n",
        "        if 'hashed_mother_id' not in Y.columns:\n",
        "            raise ValueError(\"labels missing 'hashed_mother_id' but keep_mothers_csv is set.\")\n",
        "        before = len(Y)\n",
        "        Y = Y[Y['hashed_mother_id'].astype(str).isin(keep_ids)]\n",
        "        print(f\"[INFO] labels: kept {len(Y):,}/{before:,} rows after mother filter.\")\n",
        "    keepY = [c for c in y_cols if c in Y.columns]\n",
        "    if len(keepY) != len(Y.columns):\n",
        "        Y = Y.loc[:, keepY]\n",
        "    Y = _drop_duplicate_id_columns(Y, cfg)\n",
        "    Y = _coerce_core_dtypes(Y)\n",
        "    _downcast_numeric_inplace(Y)\n",
        "\n",
        "    for name, df, keys in ((\"features\", X, TR_KEYS), (\"labels\", Y, TR_KEYS)):\n",
        "        if all(k in df.columns for k in keys):\n",
        "            n_dups = df.duplicated(keys, keep=False).sum()\n",
        "            if n_dups:\n",
        "                print(f\"[INFO] {name}: {n_dups} duplicated rows on {keys}\")\n",
        "    return X, Y\n",
        "\n",
        "def add_minutes_postpartum_by_episode(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    need = ['hashed_mother_id', 'episode_idx', 'snapshot_time']\n",
        "    for k in need:\n",
        "        if k not in X.columns:\n",
        "            raise ValueError(f\"'{k}' missing for postpartum calculation.\")\n",
        "    t0 = (\n",
        "        X.groupby(PAIR_KEYS, observed=True, sort=False)['snapshot_time']\n",
        "         .min()\n",
        "         .rename('episode_start')\n",
        "         .reset_index()\n",
        "    )\n",
        "    X = X.join(t0.set_index(PAIR_KEYS), on=PAIR_KEYS)\n",
        "    mins = (X['snapshot_time'] - X['episode_start']).dt.total_seconds() / 60.0\n",
        "    X = X.assign(mins_postpartum=mins)\n",
        "    return X\n",
        "\n",
        "def _detect_labels_time_col(Y: pd.DataFrame, cfg: Dict) -> Optional[str]:\n",
        "    if 'snapshot_time' in Y.columns: return 'snapshot_time'\n",
        "    et = cfg.get('labels_event_time_col')\n",
        "    if et and et in Y.columns: return et\n",
        "    cand = [c for c in Y.columns if 'time' in str(c).lower()]\n",
        "    return cand[0] if cand else None\n",
        "\n",
        "def apply_labels_by_intersection(X: pd.DataFrame, Y: pd.DataFrame, cfg: Dict) -> pd.Series:\n",
        "    target_col = cfg['target_col']\n",
        "    if target_col not in Y.columns:\n",
        "        raise ValueError(f\"Target column '{target_col}' not found in labels parquet\")\n",
        "    pairs = X[PAIR_KEYS].drop_duplicates()\n",
        "    Y_pairs = Y.merge(pairs, on=PAIR_KEYS, how='inner')\n",
        "    tcol = _detect_labels_time_col(Y_pairs, cfg)\n",
        "    if tcol is None:\n",
        "        lab_per_pair = (\n",
        "            Y_pairs.groupby(PAIR_KEYS, as_index=False, observed=True, sort=False)[target_col]\n",
        "                   .max()\n",
        "        )\n",
        "        X_lab = X.merge(lab_per_pair, on=PAIR_KEYS, how='left', sort=False)\n",
        "        y_arr = X_lab[target_col].to_numpy()\n",
        "        return pd.Series(y_arr, index=X.index, name=target_col).fillna(0).astype(int)\n",
        "\n",
        "    Y_pairs = Y_pairs.copy()\n",
        "    Y_pairs[tcol] = pd.to_datetime(Y_pairs[tcol], errors='coerce')\n",
        "    Y_pairs = Y_pairs.dropna(subset=[tcol])\n",
        "    if tcol != 'snapshot_time':\n",
        "        Y_pairs = Y_pairs.rename(columns={tcol: 'snapshot_time'})\n",
        "    Y_cand = (\n",
        "        Y_pairs.groupby(TR_KEYS, as_index=False, observed=True, sort=False)[target_col]\n",
        "               .max()\n",
        "    )\n",
        "    mark = X[TR_KEYS].merge(Y_cand, on=TR_KEYS, how='left', sort=False)\n",
        "    y_arr = mark[target_col].to_numpy()\n",
        "    return pd.Series(y_arr, index=X.index, name=target_col).fillna(0).astype(int)\n",
        "\n",
        "def _compute_matrices(cfg: Dict):\n",
        "    X, Y = load_feature_label_tables(cfg)\n",
        "    X = add_minutes_postpartum_by_episode(X)\n",
        "    used = [c for c in _plan_feature_columns_to_read(cfg) if c in X.columns]\n",
        "    Xdf = X[used]\n",
        "    Xdf = Xdf.replace([np.inf, -np.inf], np.nan)\n",
        "    for c in Xdf.columns:\n",
        "        if not (pd.api.types.is_float_dtype(Xdf[c]) or pd.api.types.is_integer_dtype(Xdf[c])):\n",
        "            Xdf[c] = pd.to_numeric(Xdf[c], errors='coerce')\n",
        "        Xdf[c] = Xdf[c].astype(np.float32)\n",
        "    if cfg.get('drop_constant_features', True):\n",
        "        var = Xdf.var(axis=0, numeric_only=True)\n",
        "        keep = var.index[var.values > float(cfg.get('min_feature_variance', 1e-12))].tolist()\n",
        "        if len(keep) < len(used):\n",
        "            print(f\"[INFO] Dropping {len(used) - len(keep)} near-constant features.\")\n",
        "        used = keep\n",
        "        Xdf = Xdf[used]\n",
        "    y = apply_labels_by_intersection(X, Y, cfg)\n",
        "    if cfg.get('drop_na_target', True):\n",
        "        mask = ~pd.isna(y)\n",
        "        Xdf = Xdf.loc[mask].reset_index(drop=True)\n",
        "        y = y.loc[mask].reset_index(drop=True)\n",
        "        X = X.loc[mask].reset_index(drop=True)\n",
        "    if cfg.get('target_binarize', True):\n",
        "        pos_set = set(cfg.get('positive_labels', [1, 2]))\n",
        "        y = y.apply(lambda v: 1 if v in pos_set else 0).astype(int)\n",
        "    else:\n",
        "        y = y.astype(int)\n",
        "    meta_cols = ['hashed_mother_id','episode_idx','snapshot_time','mins_postpartum','pregnancy_index']\n",
        "    meta_cols = [c for c in meta_cols if c in X.columns]\n",
        "    meta = X[meta_cols].copy()\n",
        "    del X, Y; gc.collect()\n",
        "    return Xdf, y, meta, used\n",
        "\n",
        "def build_matrices(cfg: Dict):\n",
        "    return _compute_matrices(cfg)\n"
      ],
      "id": "helpers_utils"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "metrics_plot_utils"
      },
      "outputs": [],
      "source": [
        "# ==== Metrics & plotting, datasets, TabM helpers, training loops ====\n",
        "def summarize_metrics(y_true: np.ndarray, y_prob: np.ndarray, threshold: float = 0.5) -> str:\n",
        "    y_pred = (y_prob >= threshold).astype(int)\n",
        "    auc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else float('nan')\n",
        "    ap  = average_precision_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else float('nan')\n",
        "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    spec = tn / (tn + fp) if (tn + fp) else float('nan')\n",
        "    sens = tp / (tp + fn) if (tp + fn) else float('nan')\n",
        "    return \"\\n\".join([\n",
        "        f'AUC: {auc:.4f}',\n",
        "        f'Average Precision (PR AUC): {ap:.4f}',\n",
        "        f'Precision@{threshold:.2f}: {p:.4f}',\n",
        "        f'Recall/Sensitivity@{threshold:.2f}: {r:.4f}',\n",
        "        f'Specificity@{threshold:.2f}: {spec:.4f}',\n",
        "        f'F1@{threshold:.2f}: {f1:.4f}',\n",
        "        f'Confusion Matrix @{threshold:.2f}: TN={tn} FP={fp} FN={fn} TP={tp}',\n",
        "    ])\n",
        "\n",
        "def plot_roc(y_true: np.ndarray, y_prob: np.ndarray, out_path: str):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "    auc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else float('nan')\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.plot(fpr, tpr, label=f'AUC = {auc:.3f}')\n",
        "    plt.plot([0, 1], [0, 1], '--')\n",
        "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate (Sensitivity)')\n",
        "    plt.title('ROC Curve'); plt.legend(); plt.tight_layout(); plt.savefig(out_path, dpi=160); plt.close()\n",
        "\n",
        "def plot_pr(y_true: np.ndarray, y_prob: np.ndarray, out_path: str):\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
        "    ap = average_precision_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else float('nan')\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.plot(recall, precision, label=f'AP = {ap:.3f}')\n",
        "    plt.xlabel('Recall'); plt.ylabel('Precision (PPV)')\n",
        "    plt.title('Precision–Recall Curve'); plt.legend(); plt.tight_layout(); plt.savefig(out_path, dpi=160); plt.close()\n",
        "\n",
        "def plot_confusion(y_true: np.ndarray, y_prob: np.ndarray, out_path: str, threshold: float = 0.5):\n",
        "    y_pred = (y_prob >= threshold).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    mat = np.array([[tn, fp], [fn, tp]], dtype=float)\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    plt.imshow(mat, interpolation='nearest')\n",
        "    plt.title(f'Confusion Matrix @ {threshold:.2f}')\n",
        "    plt.colorbar(); ticks = np.arange(2)\n",
        "    plt.xticks(ticks, ['Pred 0', 'Pred 1']); plt.yticks(ticks, ['True 0', 'True 1'])\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            plt.text(j, i, int(mat[i, j]), ha='center', va='center',\n",
        "                     color='white' if mat[i, j] > mat.max()/2 else 'black')\n",
        "    plt.tight_layout(); plt.ylabel('True'); plt.xlabel('Pred')\n",
        "    plt.savefig(out_path, dpi=160); plt.close()\n",
        "\n",
        "class ArrayDataset(Dataset):\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        self.X = X.astype(np.float32, copy=False)\n",
        "        self.y = y.astype(np.float32, copy=False)\n",
        "    def __len__(self): return self.X.shape[0]\n",
        "    def __getitem__(self, i: int): return self.X[i], self.y[i]\n",
        "\n",
        "# Patch: StandardizeImputer to also return a missing mask\n",
        "class StandardizeImputer:\n",
        "    def __init__(self):\n",
        "        self.median_ = None; self.mean_ = None; self.std_ = None\n",
        "    def fit(self, X: np.ndarray):\n",
        "        self.median_ = np.nanmedian(X, axis=0).astype(np.float32)\n",
        "        X_imp = np.where(np.isnan(X), self.median_, X)\n",
        "        self.mean_ = X_imp.mean(axis=0).astype(np.float32)\n",
        "        self.std_  = X_imp.std(axis=0).astype(np.float32)\n",
        "        self.std_[self.std_ < 1e-8] = 1.0\n",
        "        return self\n",
        "    def transform_with_mask(self, X: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
        "        miss = np.isnan(X).astype(np.float32)          # 1 = missing, 0 = present\n",
        "        X_imp = np.where(np.isnan(X), self.median_, X).astype(np.float32)\n",
        "        X_std = (X_imp - self.mean_) / self.std_\n",
        "        return X_std, miss\n",
        "    # keep .to_dict() / .from_dict() as-is if you already have them\n",
        "    def to_dict(self) -> Dict[str, list]:\n",
        "        return {'median': self.median_.tolist(), 'mean': self.mean_.tolist(), 'std': self.std_.tolist()}\n",
        "    @staticmethod\n",
        "    def from_dict(d: Dict[str, list]):\n",
        "        si = StandardizeImputer()\n",
        "        si.median_ = np.array(d['median'], dtype=np.float32)\n",
        "        si.mean_   = np.array(d['mean'], dtype=np.float32)\n",
        "        si.std_    = np.array(d['std'], dtype=np.float32)\n",
        "        return si\n",
        "\n",
        "def make_tabm(n_features: int, cfg: Dict) -> TabM:\n",
        "    use_emb = bool(cfg.get('use_num_embeddings', True) and _HAS_NUM_EMB)\n",
        "    if use_emb:\n",
        "        emb = LinearReLUEmbeddings(n_features)\n",
        "        model = TabM.make(n_num_features=n_features, num_embeddings=emb, d_out=1, k=int(cfg.get('tabm_k', 24)))\n",
        "    else:\n",
        "        model = TabM.make(n_num_features=n_features, d_out=1, k=int(cfg.get('tabm_k', 24)))\n",
        "    return model\n",
        "\n",
        "# --- train_epoch (replace your existing one) ---\n",
        "def train_epoch(\n",
        "    model,\n",
        "    loader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    pos_weight: float | None,\n",
        "    scheduler=None,\n",
        "    scheduler_type: str = 'none',\n",
        "    grad_clip_norm=None\n",
        "):\n",
        "    model.train()\n",
        "    loss_fn = nn.BCEWithLogitsLoss(\n",
        "        pos_weight=torch.tensor([pos_weight], device=device)\n",
        "    ) if (pos_weight and pos_weight > 0) else nn.BCEWithLogitsLoss()\n",
        "\n",
        "    total = 0.0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device); yb = yb.to(device)\n",
        "        logits_k = model(xb).squeeze(-1)            # (B, K)\n",
        "        loss = loss_fn(logits_k, yb.unsqueeze(1).expand_as(logits_k))\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        if grad_clip_norm:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), float(grad_clip_norm))\n",
        "        optimizer.step()\n",
        "\n",
        "        # per-batch step for OneCycle\n",
        "        if scheduler is not None and scheduler_type == 'onecycle':\n",
        "            scheduler.step()\n",
        "\n",
        "        total += loss.item() * xb.size(0)\n",
        "\n",
        "    return total / max(1, len(loader.dataset))\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_proba(model, loader, device) -> np.ndarray:\n",
        "    model.eval(); outs = []\n",
        "    for xb, _ in loader:\n",
        "        xb = xb.to(device)\n",
        "        logits_k = model(xb).squeeze(-1)\n",
        "        p = torch.sigmoid(logits_k).mean(dim=1)\n",
        "        outs.append(p.detach().cpu().numpy())\n",
        "    return np.concatenate(outs, axis=0)\n",
        "\n",
        "def weight_norm_importance_tabm(model: TabM, feature_names: List[str]) -> pd.DataFrame:\n",
        "    w = None\n",
        "    for n, p in model.named_parameters():\n",
        "        if 'weight' in n and p.dim() == 2 and p.shape[1] == len(feature_names):\n",
        "            w = p.detach().cpu().abs().sum(dim=0).numpy()\n",
        "            break\n",
        "    if w is None:\n",
        "        acc = None\n",
        "        for n, p in model.named_parameters():\n",
        "            if 'weight' in n and p.dim() == 2 and p.shape[1] == len(feature_names):\n",
        "                v = p.detach().cpu().abs().sum(dim=0).numpy()\n",
        "                acc = v if acc is None else acc + v\n",
        "        if acc is None:\n",
        "            acc = np.ones(len(feature_names), dtype=float)\n",
        "        w = acc\n",
        "    df = pd.DataFrame({'feature': feature_names, 'importance': w}).sort_values('importance', ascending=False).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def compute_permutation_importance(\n",
        "    model: TabM,\n",
        "    X_va: np.ndarray,\n",
        "    y_va: np.ndarray,\n",
        "    device: str,\n",
        "    feature_names: List[str],\n",
        "    n_repeats: int = 5,\n",
        "    max_features: int | None = None,\n",
        "    seed: int = 42\n",
        ") -> pd.DataFrame:\n",
        "    from sklearn.inspection import permutation_importance  # local import to avoid dependency surprises\n",
        "\n",
        "    class _Wrap:\n",
        "        def __init__(self, model, device): self.model = model; self.device = device\n",
        "        def predict_proba(self, X):\n",
        "            dl = DataLoader(\n",
        "                ArrayDataset(X.astype(np.float32), np.zeros((X.shape[0],), dtype=np.float32)),\n",
        "                batch_size=4096, shuffle=False, num_workers=0\n",
        "            )\n",
        "            p = predict_proba(self.model, dl, self.device)\n",
        "            return np.vstack([1.0 - p, p]).T\n",
        "\n",
        "    base_imp = weight_norm_importance_tabm(model, feature_names)\n",
        "    if max_features:\n",
        "        take = min(max_features, len(feature_names))\n",
        "        top_feats = base_imp.head(take)['feature'].tolist()\n",
        "        idx = np.array([feature_names.index(f) for f in top_feats], dtype=int)\n",
        "        X_sel = X_va[:, idx]; feat_sel = [feature_names[i] for i in idx]\n",
        "    else:\n",
        "        X_sel = X_va; feat_sel = feature_names\n",
        "\n",
        "    wrapper = _Wrap(model, device)\n",
        "    res = permutation_importance(\n",
        "        wrapper, X_sel, y_va, n_repeats=n_repeats,\n",
        "        scoring='average_precision', random_state=seed, n_jobs=1\n",
        "    )\n",
        "    df = pd.DataFrame({\n",
        "        'feature': feat_sel,\n",
        "        'importance_mean': res.importances_mean,\n",
        "        'importance_std': res.importances_std\n",
        "    })\n",
        "    return df.sort_values('importance_mean', ascending=False).reset_index(drop=True)\n",
        "\n",
        "def plot_feature_importance_df(imp_df: pd.DataFrame, out_path: str, top_n: int = 30, col='importance'):\n",
        "    if imp_df is None or imp_df.empty:\n",
        "        print('[WARN] No feature importances to plot.'); return\n",
        "    key = col if col in imp_df.columns else (imp_df.columns[1] if len(imp_df.columns) > 1 else None)\n",
        "    if key is None:\n",
        "        print('[WARN] Importance DF has no plottable column.'); return\n",
        "    top = imp_df.sort_values(key, ascending=False).head(top_n).iloc[::-1]\n",
        "    if top.empty:\n",
        "        print('[WARN] Top-N slice empty; skipping importance plot.'); return\n",
        "    plt.figure(figsize=(8, max(4, 0.28 * len(top))))\n",
        "    plt.barh(top['feature'], top[key])\n",
        "    plt.xlabel(key); plt.title(f'Top {len(top)} Feature Importances ({key})')\n",
        "    plt.tight_layout(); plt.savefig(out_path, dpi=160); plt.close()\n",
        "\n",
        "def _prepare_split_tensors(Xdf, y, idx, scaler: StandardizeImputer | None = None):\n",
        "    X_np = Xdf.iloc[idx].to_numpy(dtype=np.float32, copy=True)\n",
        "    y_np = y.iloc[idx].to_numpy(dtype=np.float32, copy=False)\n",
        "    if scaler is None:\n",
        "        scaler = StandardizeImputer().fit(X_np)\n",
        "    X_std, M = scaler.transform_with_mask(X_np)\n",
        "    # Concatenate standardized values + missingness mask\n",
        "    X_cat = np.concatenate([X_std, M], axis=1).astype(np.float32)\n",
        "    return X_cat, y_np, scaler\n",
        "\n",
        "\n",
        "def _downsample_to_even(X: pd.DataFrame, y: pd.Series, random_state: int = 42):\n",
        "    yb = y.astype(int).values\n",
        "    pos_idx = np.flatnonzero(yb == 1); neg_idx = np.flatnonzero(yb == 0)\n",
        "    if len(pos_idx) == 0 or len(neg_idx) == 0:\n",
        "        return X, y, np.arange(len(y))\n",
        "    if len(pos_idx) > len(neg_idx):\n",
        "        maj, mini = pos_idx, neg_idx\n",
        "    else:\n",
        "        maj, mini = neg_idx, pos_idx\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    keep_maj = rng.choice(maj, size=len(mini), replace=False)\n",
        "    keep = np.sort(np.concatenate([keep_maj, mini]))\n",
        "    return X.iloc[keep], y.iloc[keep], keep\n",
        "\n",
        "def make_holdout_masks(groups: np.ndarray, holdout_frac: float, seed: int = 42):\n",
        "    codes, uniques = pd.factorize(groups, sort=False)\n",
        "    n_groups = len(uniques)\n",
        "    if n_groups < 2:\n",
        "        va_mask = np.zeros_like(codes, dtype=bool)\n",
        "        tr_mask = ~va_mask\n",
        "        return tr_mask, va_mask\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n_val = max(1, int(math.ceil(n_groups * float(holdout_frac))))\n",
        "    val_codes = rng.choice(n_groups, size=n_val, replace=False)\n",
        "    flags = np.zeros(n_groups, dtype=bool); flags[val_codes] = True\n",
        "    va_mask = flags[codes]; tr_mask = ~va_mask\n",
        "    return tr_mask, va_mask\n",
        "\n",
        "# --- train_holdout (replace your existing one) ---\n",
        "def train_holdout(cfg: Dict, Xdf: pd.DataFrame, y: pd.Series, groups: np.ndarray, feature_names: List[str]):\n",
        "    tr_mask, va_mask = make_holdout_masks(groups, cfg['holdout_group_frac'], cfg.get('random_state', 42))\n",
        "    X_tr, y_tr = Xdf.iloc[tr_mask], y.iloc[tr_mask]\n",
        "    X_va, y_va = Xdf.iloc[va_mask], y.iloc[va_mask]\n",
        "\n",
        "    if cfg.get('downsample_training_to_even', False):\n",
        "        X_tr, y_tr, _ = _downsample_to_even(X_tr, y_tr, cfg.get('downsample_random_state', 42))\n",
        "\n",
        "    X_tr_np, y_tr_np, scaler = _prepare_split_tensors(X_tr, y_tr, np.arange(len(X_tr)))\n",
        "    X_va_np, y_va_np, _      = _prepare_split_tensors(X_va, y_va, np.arange(len(X_va)), scaler=scaler)\n",
        "\n",
        "    device = cfg.get('device', 'cpu')\n",
        "    model = make_tabm(X_tr_np.shape[1], cfg).to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])\n",
        "\n",
        "    # class imbalance handling\n",
        "    pos = float((y_tr_np == 1).sum()); neg = float((y_tr_np == 0).sum())\n",
        "    pos_weight = (neg / max(pos, 1.0)) if pos > 0 else None\n",
        "    if pos_weight is not None:\n",
        "        pos_weight = min(pos_weight, float(cfg.get('max_pos_weight', 20.0)))\n",
        "\n",
        "    # sampler (keeps all data balanced)\n",
        "    from torch.utils.data import WeightedRandomSampler\n",
        "    if cfg.get('use_weighted_sampler', True):\n",
        "        cls_counts = np.bincount(y_tr_np.astype(int), minlength=2)\n",
        "        w = torch.tensor([1.0 / cls_counts[int(t)] for t in y_tr_np], dtype=torch.double)\n",
        "        sampler = WeightedRandomSampler(w, num_samples=len(w), replacement=True)\n",
        "        train_loader = DataLoader(\n",
        "            ArrayDataset(X_tr_np, y_tr_np),\n",
        "            batch_size=cfg['batch_size'],\n",
        "            sampler=sampler,\n",
        "            num_workers=cfg['num_workers']\n",
        "        )\n",
        "    else:\n",
        "        train_loader = DataLoader(\n",
        "            ArrayDataset(X_tr_np, y_tr_np),\n",
        "            batch_size=cfg['batch_size'],\n",
        "            shuffle=True,\n",
        "            num_workers=cfg['num_workers']\n",
        "        )\n",
        "\n",
        "    val_loader   = DataLoader(\n",
        "        ArrayDataset(X_va_np, y_va_np),\n",
        "        batch_size=cfg['val_batch_size'],\n",
        "        shuffle=False,\n",
        "        num_workers=cfg['num_workers']\n",
        "    )\n",
        "\n",
        "    # --- LR scheduler ---\n",
        "    sched = None\n",
        "    sched_type = str(cfg.get('lr_scheduler', 'none')).lower()\n",
        "    if sched_type == 'onecycle':\n",
        "        from torch.optim.lr_scheduler import OneCycleLR\n",
        "        steps_per_epoch = len(train_loader)\n",
        "        sched = OneCycleLR(\n",
        "            opt, max_lr=cfg['lr'], epochs=int(cfg['epochs']), steps_per_epoch=steps_per_epoch,\n",
        "            pct_start=float(cfg.get('onecycle_pct_start', 0.3)),\n",
        "            div_factor=float(cfg.get('onecycle_div_factor', 25.0)),\n",
        "            final_div_factor=float(cfg.get('onecycle_final_div_factor', 1e4))\n",
        "        )\n",
        "    elif sched_type == 'cosine':\n",
        "        from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "        sched = CosineAnnealingLR(opt, T_max=int(cfg['epochs']), eta_min=float(cfg.get('min_lr', 1e-5)))\n",
        "    elif sched_type == 'plateau':\n",
        "        from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "        sched = ReduceLROnPlateau(\n",
        "            opt, mode='max', factor=float(cfg.get('plateau_factor', 0.5)),\n",
        "            patience=int(cfg.get('plateau_patience', 2)), min_lr=float(cfg.get('min_lr', 1e-5))\n",
        "        )\n",
        "\n",
        "    best_ap = -np.inf; best_state = None\n",
        "    patience = cfg.get('early_stop_patience', 5); no_improve = 0\n",
        "    for epoch in range(1, int(cfg['epochs']) + 1):\n",
        "        tr_loss = train_epoch(\n",
        "            model, train_loader, opt, device, pos_weight,\n",
        "            scheduler=sched, scheduler_type=sched_type,\n",
        "            grad_clip_norm=cfg.get('grad_clip_norm', None)\n",
        "        )\n",
        "\n",
        "        y_prob_va = predict_proba(model, val_loader, device)\n",
        "        ap = average_precision_score(y_va_np, y_prob_va) if len(np.unique(y_va_np)) > 1 else float('nan')\n",
        "\n",
        "        # epoch-level scheduler steps\n",
        "        if sched is not None and sched_type == 'cosine':\n",
        "            sched.step()\n",
        "        elif sched is not None and sched_type == 'plateau':\n",
        "            sched.step(ap)\n",
        "\n",
        "        cur_lr = opt.param_groups[0]['lr']\n",
        "        print(f\"[Epoch {epoch:03d}] train_loss={tr_loss:.5f}  val_AP={ap:.6f}  lr={cur_lr:.2e}\")\n",
        "\n",
        "        if np.isfinite(ap) and ap > best_ap:\n",
        "            best_ap = ap; best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}; no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "        if patience is not None and no_improve >= patience:\n",
        "            print(f\"[EarlyStop] patience reached ({patience}).\"); break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    y_prob_va = predict_proba(model, val_loader, device)\n",
        "    report = summarize_metrics(y_va_np, y_prob_va)\n",
        "\n",
        "    imp_proxy = weight_norm_importance_tabm(model, feature_names)\n",
        "    imp_perm = None\n",
        "    if cfg.get('perm_importance_enabled', False):\n",
        "        imp_perm = compute_permutation_importance(\n",
        "            model, X_va_np, y_va_np, device,\n",
        "            feature_names=feature_names,\n",
        "            n_repeats=int(cfg.get('perm_importance_n_repeats', 5)),\n",
        "            max_features=int(cfg.get('perm_importance_max_features', 200)),\n",
        "            seed=int(cfg.get('random_state', 42)),\n",
        "        )\n",
        "    return model, report, y_va_np, y_prob_va, scaler, imp_proxy, imp_perm\n",
        "\n",
        "\n",
        "# --- train_group_kfold (replace your existing one) ---\n",
        "def train_group_kfold(cfg: Dict, Xdf: pd.DataFrame, y: pd.Series, groups: np.ndarray, feature_names: List[str]):\n",
        "    gkf = GroupKFold(n_splits=cfg['group_kfold_splits'])\n",
        "    fold_reports = []\n",
        "    best_model, best_scaler, best_auc = None, None, -np.inf\n",
        "    best_yva, best_pva = None, None\n",
        "    best_imp_proxy, best_imp_perm = None, None\n",
        "    device = cfg.get('device', 'cpu')\n",
        "\n",
        "    from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(Xdf, y, groups=groups), start=1):\n",
        "        X_tr, y_tr = Xdf.iloc[tr_idx], y.iloc[tr_idx]\n",
        "        X_va, y_va = Xdf.iloc[va_idx], y.iloc[va_idx]\n",
        "\n",
        "        if cfg.get('downsample_training_to_even', False):\n",
        "            X_tr, y_tr, _ = _downsample_to_even(X_tr, y_tr, cfg.get('downsample_random_state', 42))\n",
        "\n",
        "        X_tr_np, y_tr_np, scaler = _prepare_split_tensors(X_tr, y_tr, np.arange(len(X_tr)))\n",
        "        X_va_np, y_va_np, _      = _prepare_split_tensors(X_va, y_va, np.arange(len(X_va)), scaler=scaler)\n",
        "\n",
        "        model = make_tabm(X_tr_np.shape[1], cfg).to(device)\n",
        "        opt = torch.optim.AdamW(model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])\n",
        "\n",
        "        pos = float((y_tr_np == 1).sum()); neg = float((y_tr_np == 0).sum())\n",
        "        pos_weight = (neg / max(pos, 1.0)) if pos > 0 else None\n",
        "        if pos_weight is not None:\n",
        "            pos_weight = min(pos_weight, float(cfg.get('max_pos_weight', 20.0)))\n",
        "\n",
        "        # sampler or shuffle\n",
        "        if cfg.get('use_weighted_sampler', True):\n",
        "            cls_counts = np.bincount(y_tr_np.astype(int), minlength=2)\n",
        "            w = torch.tensor([1.0 / cls_counts[int(t)] for t in y_tr_np], dtype=torch.double)\n",
        "            sampler = WeightedRandomSampler(w, num_samples=len(w), replacement=True)\n",
        "            train_loader = DataLoader(\n",
        "                ArrayDataset(X_tr_np, y_tr_np),\n",
        "                batch_size=cfg['batch_size'],\n",
        "                sampler=sampler,\n",
        "                num_workers=cfg['num_workers']\n",
        "            )\n",
        "        else:\n",
        "            train_loader = DataLoader(\n",
        "                ArrayDataset(X_tr_np, y_tr_np),\n",
        "                batch_size=cfg['batch_size'],\n",
        "                shuffle=True,\n",
        "                num_workers=cfg['num_workers']\n",
        "            )\n",
        "\n",
        "        val_loader   = DataLoader(\n",
        "            ArrayDataset(X_va_np, y_va_np),\n",
        "            batch_size=cfg['val_batch_size'],\n",
        "            shuffle=False,\n",
        "            num_workers=cfg['num_workers']\n",
        "        )\n",
        "\n",
        "        # scheduler (per fold)\n",
        "        sched = None\n",
        "        sched_type = str(cfg.get('lr_scheduler', 'none')).lower()\n",
        "        if sched_type == 'onecycle':\n",
        "            from torch.optim.lr_scheduler import OneCycleLR\n",
        "            steps_per_epoch = len(train_loader)\n",
        "            sched = OneCycleLR(\n",
        "                opt, max_lr=cfg['lr'], epochs=int(cfg['epochs']), steps_per_epoch=steps_per_epoch,\n",
        "                pct_start=float(cfg.get('onecycle_pct_start', 0.3)),\n",
        "                div_factor=float(cfg.get('onecycle_div_factor', 25.0)),\n",
        "                final_div_factor=float(cfg.get('onecycle_final_div_factor', 1e4))\n",
        "            )\n",
        "        elif sched_type == 'cosine':\n",
        "            from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "            sched = CosineAnnealingLR(opt, T_max=int(cfg['epochs']), eta_min=float(cfg.get('min_lr', 1e-5)))\n",
        "        elif sched_type == 'plateau':\n",
        "            from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "            sched = ReduceLROnPlateau(\n",
        "                opt, mode='max', factor=float(cfg.get('plateau_factor', 0.5)),\n",
        "                patience=int(cfg.get('plateau_patience', 2)), min_lr=float(cfg.get('min_lr', 1e-5))\n",
        "            )\n",
        "\n",
        "        best_ap = -np.inf; best_state = None\n",
        "        patience = cfg.get('early_stop_patience', 5); no_improve = 0\n",
        "        for epoch in range(1, int(cfg['epochs']) + 1):\n",
        "            tr_loss = train_epoch(\n",
        "                model, train_loader, opt, device, pos_weight,\n",
        "                scheduler=sched, scheduler_type=sched_type,\n",
        "                grad_clip_norm=cfg.get('grad_clip_norm', None)\n",
        "            )\n",
        "\n",
        "            y_prob_va = predict_proba(model, val_loader, device)\n",
        "            ap = average_precision_score(y_va_np, y_prob_va) if len(np.unique(y_va_np)) > 1 else float('nan')\n",
        "\n",
        "            if sched is not None and sched_type == 'cosine':\n",
        "                sched.step()\n",
        "            elif sched is not None and sched_type == 'plateau':\n",
        "                sched.step(ap)\n",
        "\n",
        "            cur_lr = opt.param_groups[0]['lr']\n",
        "            print(f\"[FOLD {fold} | Epoch {epoch:03d}] train_loss={tr_loss:.5f}  val_AP={ap:.6f}  lr={cur_lr:.2e}\")\n",
        "\n",
        "            if np.isfinite(ap) and ap > best_ap:\n",
        "                best_ap = ap; best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}; no_improve = 0\n",
        "            else:\n",
        "                no_improve += 1\n",
        "            if patience is not None and no_improve >= patience:\n",
        "                print(f\"[FOLD {fold}] EarlyStop ({patience}).\"); break\n",
        "\n",
        "        if best_state is not None:\n",
        "            model.load_state_dict(best_state)\n",
        "\n",
        "        y_prob_va = predict_proba(model, val_loader, device)\n",
        "        auc = roc_auc_score(y_va_np, y_prob_va) if len(np.unique(y_va_np)) > 1 else float('nan')\n",
        "        fold_reports.append(f\"[FOLD {fold}]\\n\" + summarize_metrics(y_va_np, y_prob_va))\n",
        "\n",
        "        if np.isfinite(auc) and auc > best_auc:\n",
        "            best_auc, best_model, best_scaler = auc, model, scaler\n",
        "            best_yva, best_pva = y_va_np, y_prob_va\n",
        "            best_imp_proxy = weight_norm_importance_tabm(model, feature_names)\n",
        "            if cfg.get('perm_importance_enabled', False):\n",
        "                best_imp_perm = compute_permutation_importance(\n",
        "                    model, X_va_np, y_va_np, device,\n",
        "                    feature_names=feature_names,\n",
        "                    n_repeats=int(cfg.get('perm_importance_n_repeats', 5)),\n",
        "                    max_features=int(cfg.get('perm_importance_max_features', 200)),\n",
        "                    seed=int(cfg.get('random_state', 42)),\n",
        "                )\n",
        "\n",
        "    assert best_model is not None, 'CV failed to produce a model.'\n",
        "    return best_model, \"\\n\\n\".join(fold_reports), best_yva, best_pva, best_scaler, best_imp_proxy, best_imp_perm\n",
        "\n"
      ],
      "id": "metrics_plot_utils"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "run_training",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae15f840-b380-46db-cb60-d111bc109db9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] keep_only_name_contains kept 226/359 columns (static always kept).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3015422043.py:195: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  X = X.assign(mins_postpartum=mins)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] keep_only_name_contains kept 226/359 columns (static always kept).\n",
            "[INFO] Dropping 28 near-constant features.\n"
          ]
        }
      ],
      "source": [
        "# ==== Run training and write outputs to Drive ====\n",
        "seed_everything(int(CFG.get('random_state', 42)))\n",
        "if CFG.get('torch_num_threads'):\n",
        "    torch.set_num_threads(CFG['torch_num_threads'])\n",
        "\n",
        "for p in (CFG['out_model_pt'], CFG['out_used_features_json'], CFG['out_metrics_txt'],\n",
        "          CFG['out_feat_csv'], CFG['out_plot_roc'], CFG['out_plot_pr'],\n",
        "          CFG['out_plot_cm'], CFG['out_plot_feat']):\n",
        "    os.makedirs(os.path.dirname(p), exist_ok=True)\n",
        "os.makedirs(CFG.get('out_hist_dir', os.path.join(CFG['base_dir'], 'top_feature_hist')), exist_ok=True)\n",
        "\n",
        "Xdf, y, meta, used_cols = build_matrices(CFG)\n",
        "assert {'hashed_mother_id','episode_idx'}.issubset(meta.columns), 'meta must contain mother/episode.'\n",
        "groups = (meta['hashed_mother_id'].astype(str) + '#' + meta['episode_idx'].astype(str)).values\n"
      ],
      "id": "run_training"
    },
    {
      "cell_type": "code",
      "source": [
        "if CFG['validation_strategy'] == 'holdout':\n",
        "    model, report, y_va, y_prob, scaler, imp_proxy, imp_perm = train_holdout(CFG, Xdf, y, groups, used_cols)\n",
        "elif CFG['validation_strategy'] == 'group_kfold':\n",
        "    model, report, y_va, y_prob, scaler, imp_proxy, imp_perm = train_group_kfold(CFG, Xdf, y, groups, used_cols)\n",
        "else:\n",
        "    raise ValueError(\"validation_strategy must be 'holdout' or 'group_kfold'\")\n",
        "\n",
        "torch.save({'state_dict': {k: v.cpu() for k, v in model.state_dict().items()}, 'used_cols': used_cols, 'scaler': scaler.to_dict()}, CFG['out_model_pt'])\n",
        "with open(CFG['out_used_features_json'], 'w') as f:\n",
        "    json.dump(used_cols, f, indent=2)\n",
        "with open(CFG['out_metrics_txt'], 'w') as f:\n",
        "    f.write(report + '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B--8PslgDOjL",
        "outputId": "33e1d662-ded2-464f-9517-7e59c170cc23"
      },
      "id": "B--8PslgDOjL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 001] train_loss=0.48865  val_AP=0.513500  lr=2.00e-03\n",
            "[Epoch 002] train_loss=0.43207  val_AP=0.530050  lr=2.00e-03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_roc(y_va, y_prob, CFG['out_plot_roc'])\n",
        "plot_pr(y_va, y_prob, CFG['out_plot_pr'])\n",
        "plot_confusion(y_va, y_prob, CFG['out_plot_cm'], threshold=0.5)"
      ],
      "metadata": {
        "id": "pZkLv41PXIx1"
      },
      "id": "pZkLv41PXIx1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if imp_proxy is not None:\n",
        "    imp_proxy.to_csv(CFG['out_feat_csv'], index=False)\n",
        "    plot_feature_importance_df(imp_proxy.rename(columns={'importance': 'importance'}), CFG['out_plot_feat'], top_n=int(CFG.get('feat_top_n', 30)), col='importance')\n",
        "if imp_perm is not None:\n",
        "    p_csv = CFG['out_feat_csv'].replace('.csv', '_perm.csv')\n",
        "    imp_perm.to_csv(p_csv, index=False)\n",
        "    p_png = CFG['out_plot_feat'].replace('.png', '_perm.png')\n",
        "    plot_feature_importance_df(imp_perm.rename(columns={'importance_mean': 'importance'}), p_png, top_n=int(CFG.get('feat_top_n', 30)), col='importance')\n",
        "\n",
        "# Optional: histograms by label for top features\n",
        "top_n = int(CFG.get('hist_top_n', 20))\n",
        "bins = int(CFG.get('hist_bins', 40))\n",
        "out_hist_dir = CFG.get('out_hist_dir', os.path.join(CFG['base_dir'], 'top_feature_hist'))\n",
        "features_to_plot = []\n",
        "if CFG.get('hist_plot_all_used', False):\n",
        "    features_to_plot = [f for f in used_cols if f in Xdf.columns]\n",
        "else:\n",
        "    if imp_proxy is not None and not imp_proxy.empty:\n",
        "        top_feats = imp_proxy.sort_values('importance', ascending=False)['feature'].tolist()\n",
        "        seen = set()\n",
        "        for f in top_feats:\n",
        "            if f in used_cols and f not in seen:\n",
        "                features_to_plot.append(f); seen.add(f)\n",
        "            if len(features_to_plot) >= top_n:\n",
        "                break\n",
        "\n",
        "def plot_feature_histograms_by_label(Xdf, y, features, out_dir, bins=40, annotate_min_count=1):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    yb = y.astype(int).values\n",
        "    pos_mask = yb == 1; neg_mask = yb == 0\n",
        "    for feat in features:\n",
        "        if feat not in Xdf.columns: continue\n",
        "        s_pos = pd.to_numeric(Xdf.loc[pos_mask, feat], errors='coerce').dropna()\n",
        "        s_neg = pd.to_numeric(Xdf.loc[neg_mask, feat], errors='coerce').dropna()\n",
        "        n_pos, n_neg = len(s_pos), len(s_neg)\n",
        "        if n_pos == 0 and n_neg == 0: continue\n",
        "        all_vals = np.concatenate([s_pos.to_numpy(), s_neg.to_numpy()])\n",
        "        bin_edges = np.histogram_bin_edges(all_vals, bins=bins)\n",
        "        widths = np.diff(bin_edges); centers = bin_edges[:-1] + widths / 2\n",
        "        cnt_pos, _ = np.histogram(s_pos, bins=bin_edges)\n",
        "        cnt_neg, _ = np.histogram(s_neg, bins=bin_edges)\n",
        "        pct_pos = (cnt_pos / max(n_pos, 1)) * 100.0\n",
        "        pct_neg = (cnt_neg / max(n_neg, 1)) * 100.0\n",
        "        plt.figure(figsize=(8.5, 4.6))\n",
        "        plt.bar(bin_edges[:-1], pct_neg, width=widths, alpha=0.5, align='edge', label=f'label=0 (non-NaN n={n_neg})')\n",
        "        plt.bar(bin_edges[:-1], pct_pos, width=widths, alpha=0.5, align='edge', label=f'label=1 (non-NaN n={n_pos})')\n",
        "        off0, off1 = -0.18, 0.18\n",
        "        for i in range(len(centers)):\n",
        "            if cnt_neg[i] >= annotate_min_count:\n",
        "                plt.text(centers[i] + off0*widths[i], pct_neg[i] + 0.3, f\"{cnt_neg[i]}\", ha='center', va='bottom', fontsize=7, rotation=90)\n",
        "            if cnt_pos[i] >= annotate_min_count:\n",
        "                plt.text(centers[i] + off1*widths[i], pct_pos[i] + 0.3, f\"{cnt_pos[i]}\", ha='center', va='bottom', fontsize=7, rotation=90)\n",
        "        plt.title(f'Histogram (% within label among non-NaNs): {feat}')\n",
        "        plt.xlabel(feat); plt.ylabel('% of samples in label (non-NaN)'); plt.legend(); plt.tight_layout()\n",
        "        out_path = os.path.join(out_dir, f\"hist_{feat.replace('/', '_')}.png\")\n",
        "        plt.savefig(out_path, dpi=160); plt.close()\n",
        "\n",
        "if features_to_plot:\n",
        "    plot_feature_histograms_by_label(Xdf, y, features_to_plot, out_hist_dir, bins=bins)\n",
        "    print('[OK] Per-class histograms written to:', out_hist_dir)\n",
        "else:\n",
        "    print('[INFO] No features selected for hist plotting.')\n",
        "\n",
        "print('[OK] Model:', CFG['out_model_pt'])\n",
        "print('[OK] Used features:', CFG['out_used_features_json'])\n",
        "print('[OK] Metrics:', CFG['out_metrics_txt'])\n",
        "print('[OK] Feature importances CSV:', CFG['out_feat_csv'])\n",
        "print('[OK] Plots:')\n",
        "print(' -', CFG['out_plot_roc'])\n",
        "print(' -', CFG['out_plot_pr'])\n",
        "print(' -', CFG['out_plot_cm'])\n",
        "print(' -', CFG['out_plot_feat'])\n",
        "print('\\n' + report)"
      ],
      "metadata": {
        "id": "bC6JOz_PDSpb"
      },
      "id": "bC6JOz_PDSpb",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}